{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Semicleaned Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Let's import some libraries to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:30.434357Z",
     "start_time": "2019-05-25T13:47:29.733267Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Let's start by reading in the titanic_train.csv file into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:33.682257Z",
     "start_time": "2019-05-25T13:47:33.669292Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'titanic_train.csv' does not exist: b'titanic_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e566ec9c06f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'titanic_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Reading titanic_train file as dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vidhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vidhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vidhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vidhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vidhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'titanic_train.csv' does not exist: b'titanic_train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('titanic_train.csv')\n",
    "# Reading titanic_train file as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:36.220565Z",
     "start_time": "2019-05-25T13:47:36.217572Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| **Variable** | **Definition**                             | **Key**                                        |\n",
    "| :----------- | :----------------------------------------- | :--------------------------------------------- |\n",
    "| survival     | Survival                                   | 0 = No, 1 = Yes                                |\n",
    "| pclass       | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
    "| sex          | Sex                                        |                                                |\n",
    "| Age          | Age in years                               |                                                |\n",
    "| sibsp        | # of siblings / spouses aboard the Titanic |                                                |\n",
    "| parch        | # of parents / children aboard the Titanic |                                                |\n",
    "| ticket       | Ticket number                              |                                                |\n",
    "| fare         | Passenger fare                             |                                                |\n",
    "| cabin        | Cabin number                               |                                                |\n",
    "| embarked     | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "### Variable Notes\n",
    "\n",
    "**pclass**: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "**sibsp**: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "**parch**: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:39.239584Z",
     "start_time": "2019-05-25T13:47:39.227648Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Let's begin some exploratory data analysis! We'll start by checking out missing data!\n",
    "\n",
    "## Missing Data\n",
    "\n",
    "We can use seaborn to create a simple heatmap to see where we are missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:41.571825Z",
     "start_time": "2019-05-25T13:47:41.451182Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isnull()\n",
    "print((train.isnull()).groupby('Age').count())\n",
    "print((train.isnull()).groupby('Cabin').count())\n",
    "print((train.isnull()).groupby('Embarked').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates a heatmap of boolean values True/False we get from train.isnull(), every yellow dash stands for a true point, that is the point where null is present there we have true. From this we can infer that lots of age information is missing and the information missing the most is cabin information and very few in embarked. Roughly about 20% of age data is missing.\n",
    "\n",
    "- As age data missing proportionately is likely small enough for us to be able to replace it in some manner of computation, we can use knowledge from other columns to fill in the missing values of age column in a reasonable manner.\n",
    "\n",
    "- However, on seeing cabin column we are missing too much of the data to do something useful out of it. We might as well drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:43.483497Z",
     "start_time": "2019-05-25T13:47:43.480506Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:44.150444Z",
     "start_time": "2019-05-25T13:47:44.062680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting an idea about ratio of actual target variables of survival on train data\n",
    "sns.countplot(x='Survived',data=train)\n",
    "# Roughly speaking closer to 550 people perished and 300 something people survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:45.026882Z",
     "start_time": "2019-05-25T13:47:44.925154Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Seeing survival as per gender\n",
    "sns.countplot(x='Survived',hue=\"Sex\",data=train,palette=\"RdBu_r\")\n",
    "# Looks like Jack did made a sacrifice. Clearly more women survived when compared to men onboard.\n",
    "# We will see more about these when we analyse factors which play a crucial role in someone's survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:46.111205Z",
     "start_time": "2019-05-25T13:47:45.991485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualising as per Passenger class to compare people of which class survived more\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')\n",
    "# We can infer that people who perished among those there are more probability of those belongging to class 3.\n",
    "# People who survived had more probability of being from class 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:47.701999Z",
     "start_time": "2019-05-25T13:47:47.542389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get an idea about age of people onboard : For this we can do a distribution type plot\n",
    "sns.distplot(train['Age'].dropna(),kde=True,color='darkred',bins=30)\n",
    "#  What we can infer is that this appears to be bimodal distribution, there are quite a few passengers in age range of 0-10,\n",
    "#  after that there is an average age between 20-30 and as older you get number of passengers decreases onboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:48.779524Z",
     "start_time": "2019-05-25T13:47:48.621946Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:49.764613Z",
     "start_time": "2019-05-25T13:47:49.757639Z"
    }
   },
   "outputs": [],
   "source": [
    "# To get basic idea about columns\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:51.515079Z",
     "start_time": "2019-05-25T13:47:51.404405Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have seen Survived column, Sex, Age, Pclass column already, let's check SibSp - Siblings/Spouse onboard column\n",
    "sns.countplot(x='SibSp',data=train)\n",
    "# Looking at this plot we can see that around 600 had 0 siblings or spouse onboard, closer to 200 people had 1 of the sibling\n",
    "# or spouse onboard. Very few numbers of siblings/spouse for other 91 passengers in train dataset.\n",
    "# Thus, there were a lot of single people onboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:47:52.601371Z",
     "start_time": "2019-05-25T13:47:52.452770Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='SibSp',data=train,hue='Survived',palette=\"RdBu_r\")\n",
    "# Comparing amongst SibSp column who survived or not we see that survival factor is more for people who were not travelling \n",
    "# alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have explored the data, got some idea about nature of the dataset, how columns realte with survival rate in some cases, now we can move towards Data Cleaning and Building a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Cleaning\n",
    "We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\n",
    "However we can be smarter about this and check the average age by passenger class. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:01.698926Z",
     "start_time": "2019-05-25T13:48:01.549290Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.boxplot(x='Pclass',y='Age',data=train)\n",
    "# We can see that upon separation by class the wealthier passengers in the first and second class tend to be actually a bit more\n",
    "# older than the passengers of third class. We can use these average age vallues to impute the age based off of passenger class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:03.508065Z",
     "start_time": "2019-05-25T13:48:03.504113Z"
    }
   },
   "outputs": [],
   "source": [
    "#  We may create an entire model to predict age based off of other features.\n",
    "def impute_age(cols):\n",
    "    Age = cols[0] # First item in cols\n",
    "    Pclass = cols[1] # Second item\n",
    "    \n",
    "    if pd.isnull(Age):\n",
    "        \n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else: # Not in first or second class means it is third class\n",
    "            return 24\n",
    "    else:\n",
    "        #If age is null then we return some int based upon average age of that Pclass but if it is not then we do following :\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:04.948874Z",
     "start_time": "2019-05-25T13:48:04.924939Z"
    }
   },
   "outputs": [],
   "source": [
    "train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I did above ?\n",
    "\n",
    "- Created a function that takes in an argument cols, based off of index values we grab Age and Pclass, as we are using .apply() on Age and Pclass columns then we search for condition where pd.isnull(Age) is true then we look for the Pclass for that entry, and we return the average age of that class based off of Pclass and so on for second and third class. \n",
    "\n",
    "- If it is not null then we know the age and return the actual age.\n",
    "\n",
    "- axis = 1 for applying on columns.\n",
    "\n",
    "## Checking Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:08.908554Z",
     "start_time": "2019-05-25T13:48:08.796852Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "# This means that now we do not have any missing information for age column. We have successfully put in age values based off\n",
    "# of people's class's average./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:10.393308Z",
     "start_time": "2019-05-25T13:48:10.389355Z"
    }
   },
   "outputs": [],
   "source": [
    "# About cabin column, as there are too many missing points in here it is easier to drop the cabin column.\n",
    "train.drop(\"Cabin\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:11.016642Z",
     "start_time": "2019-05-25T13:48:11.005673Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:12.328322Z",
     "start_time": "2019-05-25T13:48:12.222605Z"
    }
   },
   "outputs": [],
   "source": [
    "# We no longer have cabin column and now upon running heatmap again\n",
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:13.450830Z",
     "start_time": "2019-05-25T13:48:13.444847Z"
    }
   },
   "outputs": [],
   "source": [
    "#As embarked has only a row or two of missing values we drop it\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:14.281590Z",
     "start_time": "2019-05-25T13:48:14.132983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(train.isnull(),cmap='viridis',yticklabels=False,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:15.351256Z",
     "start_time": "2019-05-25T13:48:15.340285Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T11:30:46.610362Z",
     "start_time": "2019-05-25T11:30:46.607403Z"
    }
   },
   "source": [
    "- Now we do not have any missing values. We filled in some of the missing values and dropped a very few. Now data is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical Features \n",
    "\n",
    "We'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs. What we mean to say that for columns like Sex we have male or female, for ML algorithm to understand this we have to assign them a number say 0 for male and 1 for female, this is called creating a dummy variable and we would be doing the same for Embarked columns like Q,S,C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:20.326783Z",
     "start_time": "2019-05-25T13:48:20.318862Z"
    }
   },
   "outputs": [],
   "source": [
    "# To do this we use get_dummies method() of pandas which converts categorical variables into dummy or indicator variables.\n",
    "\n",
    "dummy = pd.get_dummies(train['Sex']) # Passing in the column we want to convert and gives us a dataframe with col for every category.\n",
    "# Here 0 in female column means there would be a 1 in male colummn for that entry as it can be male or female here.\n",
    "# 1 in female column would mean this entry was a female and the male corresponding column would show 0.\n",
    "\n",
    "dummy.head()\n",
    "\n",
    "# The issue with what we get here is that in the one column is a perfect predictor of the other column, meaning if our\n",
    "# ML algorithm gets fed the columns then ML algorithm will immediately know that if it's 0 at female then it \n",
    "# will predict perfectly that it is 1 at male. This is going to be an issue called multi-collinearity. It will mess up the\n",
    "# algorithm as bunch of columns will be perfect predictors of other columns. To avoid this we type drop_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:21.394157Z",
     "start_time": "2019-05-25T13:48:21.390172Z"
    }
   },
   "outputs": [],
   "source": [
    "sex = pd.get_dummies(train['Sex'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:21.749049Z",
     "start_time": "2019-05-25T13:48:21.743064Z"
    }
   },
   "outputs": [],
   "source": [
    "sex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:22.994263Z",
     "start_time": "2019-05-25T13:48:22.990309Z"
    }
   },
   "outputs": [],
   "source": [
    "embark = pd.get_dummies(train['Embarked'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:23.363558Z",
     "start_time": "2019-05-25T13:48:23.357574Z"
    }
   },
   "outputs": [],
   "source": [
    "embark.head()\n",
    "# We see that dropping C would result in current indicators being present of Q and S and as these are not perfect indicators\n",
    "# of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:24.483989Z",
     "start_time": "2019-05-25T13:48:24.478044Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,sex,embark],axis=1) # To add new indicator variable or dummy to dataframe via concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:25.539809Z",
     "start_time": "2019-05-25T13:48:25.527841Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:27.001228Z",
     "start_time": "2019-05-25T13:48:26.997278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can drop columns like Name, Ticket, Embarked and Sex\n",
    "train.drop(['Sex','Embarked','Name','Ticket','PassengerId'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:48:30.195402Z",
     "start_time": "2019-05-25T13:48:30.185461Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:49:53.554289Z",
     "start_time": "2019-05-25T13:49:53.549302Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train,sex,embark],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:49:57.733076Z",
     "start_time": "2019-05-25T13:49:57.723141Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our data is ready for our model!\n",
    "\n",
    "# Building a Logistic Regression model\n",
    "\n",
    "Let's start by splitting our data into a training set and test set (there is another test.csv file that you can play around with in case you want to use all this data for training).\n",
    "\n",
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:14.373775Z",
     "start_time": "2019-05-25T13:50:13.977532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:18.326675Z",
     "start_time": "2019-05-25T13:50:18.320689Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(\"Survived\",axis=1)\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:29.733269Z",
     "start_time": "2019-05-25T13:50:29.728316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test= train_test_split(X, y,test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:59:13.050936Z",
     "start_time": "2019-05-25T12:59:13.045948Z"
    }
   },
   "source": [
    "## Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:31.158808Z",
     "start_time": "2019-05-25T13:50:31.119900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:31.556414Z",
     "start_time": "2019-05-25T13:50:31.552425Z"
    }
   },
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:31.997967Z",
     "start_time": "2019-05-25T13:50:31.990955Z"
    }
   },
   "outputs": [],
   "source": [
    "logmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:33.571748Z",
     "start_time": "2019-05-25T13:50:33.567758Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to evaluate our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We can check precision,recall,f1-score using classification report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:46.620248Z",
     "start_time": "2019-05-25T13:50:46.617293Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:47.221667Z",
     "start_time": "2019-05-25T13:50:47.215691Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:48.818333Z",
     "start_time": "2019-05-25T13:50:48.815340Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T13:50:49.307832Z",
     "start_time": "2019-05-25T13:50:49.302846Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so bad! You might want to explore other feature engineering and the other titanic_text.csv file, some suggestions for feature engineering:\n",
    "\n",
    "* Try grabbing the Title (Dr.,Mr.,Mrs,etc..) from the name as a feature\n",
    "* Maybe the Cabin letter could be a feature\n",
    "* Is there any info you can get from the ticket?\n",
    "\n",
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
